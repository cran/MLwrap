% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/plotting_utils.R
\name{plot_calibration_curve}
\alias{plot_calibration_curve}
\title{Plotting Calibration Curve}
\usage{
plot_calibration_curve(analysis_object)
}
\arguments{
\item{analysis_object}{Fitted analysis_object with 'fine_tuning()'.}
}
\value{
analysis_object
}
\description{
The \strong{plot_calibration_curve()} function generates calibration plots for
binary classification models evaluating the agreement between predicted
probabilities and observed class frequencies in binned prediction intervals.
Implements reliability diagrams comparing empirical success rates within
each probability bin against the predicted probability levels, identifying
systematic calibration errors including overconfidence (predicted
probabilities exceed observed frequencies) and underconfidence patterns
across prediction ranges.
}
\examples{
# Note: For obtaining the calibration curve plot the user needs to
# complete till fine_tuning( ) function of the MLwrap pipeline and
# only with binary outcome.

wrap_object <- preprocessing(df = sim_data[1:300 ,],
                             formula = psych_well_bin ~ depression + resilience,
                             task = "classification")
wrap_object <- build_model(wrap_object, "Random Forest",
                           hyperparameters = list(mtry = 2, trees = 5))
set.seed(123) # For reproducibility
wrap_object <- fine_tuning(wrap_object, "Grid Search CV")

# And then, you can obtain the calibration curve plot.

plot_calibration_curve(wrap_object)
}
